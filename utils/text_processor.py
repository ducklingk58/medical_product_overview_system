from typing import List, Dict, Any, Tuple, Optional
from llm.ollama_client import generate_overview_with_llm, test_ollama_connection
import re
import json
from collections import Counter
import unicodedata

def clean_foreign_languages(text: str) -> str:
    """ì™¸êµ­ì–´, í•œì, íŠ¹ìˆ˜ê¸°í˜¸ë¥¼ ì œê±°í•˜ê³  í•œêµ­ì–´ë§Œ ë‚¨ê¹ë‹ˆë‹¤."""
    if not text:
        return ""
    
    # í•œì ì œê±° (CJK Unified Ideographs)
    text = re.sub(r'[\u4e00-\u9fff]', '', text)
    
    # ì¼ë³¸ì–´ íˆë¼ê°€ë‚˜ ì œê±°
    text = re.sub(r'[\u3040-\u309f]', '', text)
    
    # ì¼ë³¸ì–´ ê°€íƒ€ì¹´ë‚˜ ì œê±°
    text = re.sub(r'[\u30a0-\u30ff]', '', text)
    
    # íƒœêµ­ì–´ ì œê±°
    text = re.sub(r'[\u0e00-\u0e7f]', '', text)
    
    # ì•„ëì–´ ì œê±°
    text = re.sub(r'[\u0600-\u06ff]', '', text)
    
    # ê·¸ë¦¬ìŠ¤ì–´ ì œê±°
    text = re.sub(r'[\u0370-\u03ff]', '', text)
    
    # ëŸ¬ì‹œì•„ì–´ ì œê±°
    text = re.sub(r'[\u0400-\u04ff]', '', text)
    
    # íŠ¹ìˆ˜ê¸°í˜¸ ì œê±° (ì˜ì•½í’ˆ ê´€ë ¨ ê¸°í˜¸ëŠ” ì œì™¸)
    text = re.sub(r'[^\w\sê°€-í£ã„±-ã…ã…-ã…£\-\.\,\:\;\(\)\[\]\{\}\+\-\=\*\/\@\#\$\%\&\?\!]', '', text)
    
    # ì—°ì†ëœ ê³µë°± ì œê±°
    text = re.sub(r'\s+', ' ', text)
    
    # ì•ë’¤ ê³µë°± ì œê±°
    text = text.strip()
    
    return text

def is_korean_text(text: str) -> bool:
    """í…ìŠ¤íŠ¸ê°€ í•œêµ­ì–´ì¸ì§€ í™•ì¸í•©ë‹ˆë‹¤."""
    if not text:
        return False
    
    # í•œê¸€ ë¬¸ì ë¹„ìœ¨ ê³„ì‚°
    korean_chars = len(re.findall(r'[ê°€-í£ã„±-ã…ã…-ã…£]', text))
    total_chars = len(text.replace(' ', ''))
    
    if total_chars == 0:
        return False
    
    return korean_chars / total_chars > 0.3  # 30% ì´ìƒì´ í•œê¸€ì´ë©´ í•œêµ­ì–´ë¡œ ê°„ì£¼

def split_into_tokens(text: str) -> List[str]:
    """í…ìŠ¤íŠ¸ë¥¼ í† í°(ë‹¨ì–´/ê¸€ì) ë‹¨ìœ„ë¡œ ë¶„í• í•©ë‹ˆë‹¤."""
    # í•œê¸€, ì˜ë¬¸, ìˆ«ì, íŠ¹ìˆ˜ë¬¸ìë¥¼ ëª¨ë‘ í¬í•¨í•˜ì—¬ í† í°í™”
    # í•œê¸€: ì™„ì„±í˜• í•œê¸€ ë¬¸ì
    # ì˜ë¬¸: ì•ŒíŒŒë²³
    # ìˆ«ì: 0-9
    # íŠ¹ìˆ˜ë¬¸ì: ì˜ì•½í’ˆ ê´€ë ¨ íŠ¹ìˆ˜ë¬¸ì (mg, g, ml, %, ë“±)
    
    # ë¨¼ì € ì˜ì•½í’ˆ ê´€ë ¨ íŠ¹ìˆ˜ íŒ¨í„´ì„ ë³´í˜¸
    protected_patterns = [
        r'\d+\.?\d*\s*(mg|g|ml|mcg|IU|ë‹¨ìœ„|ì •|ìº¡ìŠ|ì£¼ì‚¬ì œ)',
        r'\d+%',
        r'USP|EP|JP',
        r'í•„ë¦„ì½”íŒ…ì •',
        r'ì•„ì„¸íŠ¸ì•„ë¯¸ë…¸íœ|ì´ë¶€í”„ë¡œíœ|ì•„ìŠ¤í”¼ë¦°|ì„¸ë§ˆê¸€ë£¨íƒ€ì´ë“œ|ë©”íŠ¸í¬ë¥´ë¯¼|ê¸€ë¦¬ë©”í”¼ë¦¬ë“œ|íŒŒì„¸íƒ€ë¯¼'
    ]
    
    # ë³´í˜¸í•  íŒ¨í„´ë“¤ì„ ì„ì‹œ ë§ˆì»¤ë¡œ êµì²´
    protected_tokens = []
    token_to_marker = {}
    
    for i, pattern in enumerate(protected_patterns):
        matches = re.finditer(pattern, text, re.IGNORECASE)
        for match in matches:
            marker = f"__PROTECTED_{i}_{len(protected_tokens)}__"
            token_to_marker[marker] = match.group()
            protected_tokens.append(match.group())
    
    # ë³´í˜¸ëœ íŒ¨í„´ë“¤ì„ ë§ˆì»¤ë¡œ êµì²´
    for marker, token in token_to_marker.items():
        text = text.replace(token, marker)
    
    # ì¼ë°˜ì ì¸ í† í°í™” (ê³µë°±, êµ¬ë‘ì  ê¸°ì¤€)
    tokens = re.split(r'[\s\.,;:!?()\[\]{}"\']+', text)
    
    # ë¹ˆ í† í° ì œê±° ë° ì •ë¦¬
    cleaned_tokens = []
    for token in tokens:
        token = token.strip()
        if token and len(token) >= 1:  # 1ê¸€ì ì´ìƒì¸ í† í°ë§Œ í¬í•¨
            cleaned_tokens.append(token)
    
    # ë³´í˜¸ëœ í† í°ë“¤ì„ ì›ë˜ ê°’ìœ¼ë¡œ ë³µì›
    final_tokens = []
    for token in cleaned_tokens:
        if token.startswith("__PROTECTED_"):
            # ë§ˆì»¤ì—ì„œ ì›ë˜ í† í° ë³µì›
            if token in token_to_marker:
                final_tokens.append(token_to_marker[token])
            else:
                final_tokens.append(token)
        else:
            final_tokens.append(token)
    
    return final_tokens

def extract_keywords_from_tokens(tokens: List[str]) -> List[str]:
    """í† í° ë¦¬ìŠ¤íŠ¸ì—ì„œ ì˜ì•½í’ˆ ê´€ë ¨ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    keywords = []
    
    # ì˜ì•½í’ˆ ê´€ë ¨ í‚¤ì›Œë“œ íŒ¨í„´
    medical_patterns = [
        r'\b\d+\.?\d*\s*(mg|g|ml|mcg|IU|ë‹¨ìœ„|ì •|ìº¡ìŠ|ì£¼ì‚¬ì œ)\b',
        r'\b(ì •ì œ|ì£¼ì‚¬ì œ|ìº¡ìŠ|ì‹œëŸ½|ì—°ê³ |í¬ë¦¼|ì•¡ì œ|ë¶„ë§|ê³¼ë¦½|ì •|í•„ë¦„ì½”íŒ…ì •)\b',
        r'\b(ì•„ì„¸íŠ¸ì•„ë¯¸ë…¸íœ|ì´ë¶€í”„ë¡œíœ|ì•„ìŠ¤í”¼ë¦°|ì„¸ë§ˆê¸€ë£¨íƒ€ì´ë“œ|ë©”íŠ¸í¬ë¥´ë¯¼|ê¸€ë¦¬ë©”í”¼ë¦¬ë“œ|íŒŒì„¸íƒ€ë¯¼)\b',
        r'\b(USP|EP|JP|ìˆœë„|í•¨ëŸ‰|ì„±ë¶„|ì£¼ì„±ë¶„|ì²¨ê°€ì œ|ê²°í•©ì œ|í¬ì„ì œ)\b',
        r'\b(ìš©ê¸°|í¬ì¥|ë³‘|ì•°í”Œ|ë°”ì´ì•Œ|í”Œë¼ìŠ¤í‹±|ìœ ë¦¬|ì•Œë£¨ë¯¸ëŠ„|ë¸”ë¦¬ìŠ¤í„°|í¬ì¼)\b',
        r'\b(ìš©ì¶œ|ë¶•í•´|ìƒì²´ì´ìš©ë¥ |ì•ˆì •ì„±|ì„±ëŠ¥|íŠ¹ì„±|ë¶„í•´|ìš©í•´ë„)\b',
        r'\b(ë³´ì¡´ì œ|ë©¸ê· |ë¯¸ìƒë¬¼|ë¬´ê· |ì‚´ê· |ë°©ë¶€ì œ)\b',
        r'\b(ê°œë°œ|ì—°êµ¬|ì œí˜•ê°œë°œ|ì²˜ë°©ê°œë°œ|ì„ íƒê·¼ê±°|ê°œë°œê·¼ê±°)\b',
        r'\b(ì™¸í˜•|ëª¨ì–‘|ìƒ‰ìƒ|ìƒ‰ê¹”|í°ìƒ‰|ë…¸ë€ìƒ‰|ê°ì¸|í‘œì‹œ|ë§ˆí¬)\b',
        r'\b(ê²½êµ¬ìš©|ì£¼ì‚¬ìš©|ì™¸ìš©|ë‚´ìš©|íˆ¬ì—¬|ë³µìš©)\b',
    ]
    
    # ê° í† í°ì„ ê²€ì‚¬
    for token in tokens:
        token_lower = token.lower()
        
        # íŒ¨í„´ ë§¤ì¹­
        for pattern in medical_patterns:
            if re.search(pattern, token, re.IGNORECASE):
                keywords.append(token)
                break
        
        # ì¼ë°˜ì ì¸ ì˜ì•½í’ˆ ê´€ë ¨ ë‹¨ì–´ë“¤
        medical_words = [
            'ì œí’ˆëª…', 'ì œí˜•', 'í•¨ëŸ‰', 'ì„±ë¶„', 'ìš©ê¸°', 'í¬ì¥', 'ìš©ì¶œ', 'ë¶•í•´',
            'ì•ˆì •ì„±', 'ë³´ì¡´ì œ', 'ë©¸ê· ', 'ê°œë°œ', 'ì—°êµ¬', 'íŠ¹ì„±', 'ì„±ëŠ¥', 'ì™¸í˜•',
            'íˆ¬ì—¬', 'ë³µìš©', 'ê²½êµ¬', 'ì£¼ì‚¬', 'ì™¸ìš©', 'ë‚´ìš©', 'ì •ì œ', 'ì£¼ì‚¬ì œ',
            'ìº¡ìŠ', 'ì‹œëŸ½', 'ì—°ê³ ', 'í¬ë¦¼', 'ì•¡ì œ', 'ë¶„ë§', 'ê³¼ë¦½', 'ì •',
            'í•„ë¦„ì½”íŒ…ì •', 'ì•„ì„¸íŠ¸ì•„ë¯¸ë…¸íœ', 'ì´ë¶€í”„ë¡œíœ', 'ì•„ìŠ¤í”¼ë¦°', 'ì„¸ë§ˆê¸€ë£¨íƒ€ì´ë“œ',
            'ë©”íŠ¸í¬ë¥´ë¯¼', 'ê¸€ë¦¬ë©”í”¼ë¦¬ë“œ', 'íŒŒì„¸íƒ€ë¯¼', 'USP', 'EP', 'JP', 'ìˆœë„',
            'ì£¼ì„±ë¶„', 'ì²¨ê°€ì œ', 'ê²°í•©ì œ', 'í¬ì„ì œ', 'ë³‘', 'ì•°í”Œ', 'ë°”ì´ì•Œ',
            'í”Œë¼ìŠ¤í‹±', 'ìœ ë¦¬', 'ì•Œë£¨ë¯¸ëŠ„', 'ë¸”ë¦¬ìŠ¤í„°', 'í¬ì¼', 'ìƒì²´ì´ìš©ë¥ ',
            'ë¶„í•´', 'ìš©í•´ë„', 'ë¯¸ìƒë¬¼', 'ë¬´ê· ', 'ì‚´ê· ', 'ë°©ë¶€ì œ', 'ì œí˜•ê°œë°œ',
            'ì²˜ë°©ê°œë°œ', 'ì„ íƒê·¼ê±°', 'ê°œë°œê·¼ê±°', 'ìƒ‰ìƒ', 'ìƒ‰ê¹”', 'í°ìƒ‰', 'ë…¸ë€ìƒ‰',
            'ê°ì¸', 'í‘œì‹œ', 'ë§ˆí¬', 'ì›í˜•', 'íƒ€ì›í˜•', 'ì„ìƒ', 'ì‹œí—˜', 'í¡ìˆ˜',
            'ë°°ì„¤', 'ì„¸ê· ', 'ê· '
        ]
        
        if token_lower in medical_words:
            keywords.append(token)
        
        # ìˆ«ìê°€ í¬í•¨ëœ í† í° (í•¨ëŸ‰ ì •ë³´)
        if re.search(r'\d', token):
            keywords.append(token)
        
        # í•œê¸€ ì˜ì•½í’ˆ ê´€ë ¨ ë‹¨ì–´ (2ê¸€ì ì´ìƒ)
        if len(token) >= 2 and re.search(r'[ê°€-í£]', token):
            # ì˜ì•½í’ˆ ê´€ë ¨ í•œê¸€ ë‹¨ì–´ë“¤
            korean_medical_words = [
                'ì œí’ˆ', 'ì œí˜•', 'í•¨ëŸ‰', 'ì„±ë¶„', 'ìš©ê¸°', 'í¬ì¥', 'ìš©ì¶œ', 'ë¶•í•´',
                'ì•ˆì •', 'ë³´ì¡´', 'ë©¸ê· ', 'ê°œë°œ', 'ì—°êµ¬', 'íŠ¹ì„±', 'ì„±ëŠ¥', 'ì™¸í˜•',
                'íˆ¬ì—¬', 'ë³µìš©', 'ê²½êµ¬', 'ì£¼ì‚¬', 'ì™¸ìš©', 'ë‚´ìš©', 'ì •ì œ', 'ì£¼ì‚¬',
                'ìº¡ìŠ', 'ì‹œëŸ½', 'ì—°ê³ ', 'í¬ë¦¼', 'ì•¡ì œ', 'ë¶„ë§', 'ê³¼ë¦½', 'ì •',
                'í•„ë¦„', 'ì½”íŒ…', 'ìˆœë„', 'ì£¼ì„±', 'ì²¨ê°€', 'ê²°í•©', 'í¬ì„', 'ë³‘',
                'ì•°í”Œ', 'ë°”ì´ì•Œ', 'í”Œë¼ìŠ¤í‹±', 'ìœ ë¦¬', 'ì•Œë£¨ë¯¸ëŠ„', 'ë¸”ë¦¬ìŠ¤í„°',
                'í¬ì¼', 'ìƒì²´', 'ì´ìš©ë¥ ', 'ë¶„í•´', 'ìš©í•´', 'ë¯¸ìƒë¬¼', 'ë¬´ê· ',
                'ì‚´ê· ', 'ë°©ë¶€', 'ì œí˜•', 'ì²˜ë°©', 'ì„ íƒ', 'ê·¼ê±°', 'ìƒ‰ìƒ', 'ìƒ‰ê¹”',
                'í°ìƒ‰', 'ë…¸ë€ìƒ‰', 'ê°ì¸', 'í‘œì‹œ', 'ë§ˆí¬', 'ì›í˜•', 'íƒ€ì›í˜•',
                'ì„ìƒ', 'ì‹œí—˜', 'í¡ìˆ˜', 'ë°°ì„¤', 'ì„¸ê· ', 'ê· ', 'ì•„ì„¸íŠ¸', 'ì•„ë¯¸ë…¸',
                'íœ', 'ì´ë¶€', 'í”„ë¡œ', 'íœ', 'ì•„ìŠ¤í”¼', 'ë¦°', 'ì„¸ë§ˆ', 'ê¸€ë£¨',
                'íƒ€ì´ë“œ', 'ë©”íŠ¸', 'í¬ë¥´ë¯¼', 'ê¸€ë¦¬', 'ë©”í”¼', 'ë¦¬ë“œ', 'íŒŒì„¸',
                'íƒ€ë¯¼'
            ]
            
            if token in korean_medical_words:
                keywords.append(token)
    
    # ì¤‘ë³µ ì œê±° ë° ì •ë¦¬
    unique_keywords = list(set(keywords))
    return [kw.strip() for kw in unique_keywords if len(kw.strip()) >= 1]

def classify_keywords_by_section(keywords: List[str]) -> Dict[str, List[str]]:
    """ì¶”ì¶œëœ í‚¤ì›Œë“œë¥¼ 7ê°œ í•­ëª© ì¤‘ í•˜ë‚˜ì— ë¶„ë¥˜í•©ë‹ˆë‹¤ (ê°€ì¤‘ì¹˜ ê¸°ë°˜)."""
    
    # ê° í•­ëª©ë³„ ëŒ€í‘œ í‚¤ì›Œë“œ ì„¸íŠ¸ ì •ì˜ (í•œêµ­ ì˜ì•½í’ˆ ì„¤ëª…ì„œ êµ¬ì¡°ì— ìµœì í™”)
    section_keyword_sets = {
        "ì„±ë¶„ ë° í•¨ëŸ‰": {
            "keywords": ["ì„±ë¶„", "ì£¼ì„±ë¶„", "ì²¨ê°€ì œ", "ê²°í•©ì œ", "í¬ì„ì œ", "ì•„ì„¸íŠ¸ì•„ë¯¸ë…¸íœ", "ì´ë¶€í”„ë¡œíœ", "ì•„ìŠ¤í”¼ë¦°", "ì„¸ë§ˆê¸€ë£¨íƒ€ì´ë“œ", "ë©”íŠ¸í¬ë¥´ë¯¼", "íŒŒì„¸íƒ€ë¯¼", "USP", "EP", "JP", "ìˆœë„", "ì•„ì„¸íŠ¸", "ì•„ë¯¸ë…¸", "íœ", "ì´ë¶€", "í”„ë¡œ", "ì„¸ë§ˆ", "ê¸€ë£¨", "íƒ€ì´ë“œ", "ë©”íŠ¸", "í¬ë¥´ë¯¼", "ê¸€ë¦¬", "ë©”í”¼", "ë¦¬ë“œ", "íŒŒì„¸", "íƒ€ë¯¼", "mg", "g", "ml", "mcg", "ë‹¨ìœ„", "í•¨ëŸ‰", "ë°°í•©ëª©ì ", "ì£¼ì„±ë¶„", "ì²¨ê°€ì„±ë¶„", "ë³´ì¡°ì„±ë¶„", "ê¸°ì¤€", "ë¶„ëŸ‰", "íˆ¬ì—¬ë‹¨ìœ„", "í•¨ëŸ‰", "ìˆœë„", "í•¨ìœ ëŸ‰", "í¬í•¨ëŸ‰", "ê·œê²©"],
            "weight": 1.2
        },
        "ì„±ìƒ": {
            "keywords": ["ì™¸í˜•", "ëª¨ì–‘", "ìƒ‰ìƒ", "ìƒ‰ê¹”", "í°ìƒ‰", "ë…¸ë€ìƒ‰", "ê°ì¸", "í‘œì‹œ", "ë§ˆí¬", "ì›í˜•", "íƒ€ì›í˜•", "ì •ì‚¬ê°í˜•", "ì§ì‚¬ê°í˜•", "ë¬´ìƒ‰", "íˆ¬ëª…", "ë¶ˆíˆ¬ëª…", "ì™¸ê´€", "í˜•íƒœ", "í¬ê¸°", "ë¬´ê²Œ", "ë‘ê»˜", "ì§€ë¦„", "ê¸¸ì´", "í­", "ì„±ìƒ"],
            "weight": 1.0
        },
        "íš¨ëŠ¥ ë° íš¨ê³¼": {
            "keywords": ["íš¨ëŠ¥", "íš¨ê³¼", "ì‘ìš©", "ì•½ë¦¬ì‘ìš©", "ì¹˜ë£Œíš¨ê³¼", "ì¹˜ë£Œì‘ìš©", "ì•½íš¨", "íš¨ê³¼ì„±", "ì¹˜ë£Œ", "ê°œì„ ", "ì™„í™”", "í•´ì—´", "ì§„í†µ", "ì†Œì—¼", "í•­ì—¼ì¦", "í•­ìƒ", "í•­ê· ", "í•­ë°”ì´ëŸ¬ìŠ¤", "í•­ì•”", "í•­ê³ í˜ˆì••", "í•­ë‹¹ë‡¨", "í•­í˜ˆì „"],
            "weight": 1.1
        },
        "ìš©ë²• ë° ìš©ëŸ‰": {
            "keywords": ["ìš©ë²•", "ìš©ëŸ‰", "íˆ¬ì—¬", "ë³µìš©", "íˆ¬ì—¬ëŸ‰", "ë³µìš©ëŸ‰", "íˆ¬ì—¬ë°©ë²•", "ë³µìš©ë°©ë²•", "íˆ¬ì—¬ê°„ê²©", "ë³µìš©ê°„ê²©", "íˆ¬ì—¬ê¸°ê°„", "ë³µìš©ê¸°ê°„", "ì ì‘ì¦", "íˆ¬ì—¬ê²½ë¡œ", "ë³µìš©ê²½ë¡œ", "ê²½êµ¬", "ì£¼ì‚¬", "ì ì ", "ë„í¬", "í¡ì…"],
            "weight": 1.1
        },
        "ì‚¬ìš©ìƒ ì£¼ì˜ì‚¬í•­": {
            "keywords": ["ì£¼ì˜ì‚¬í•­", "ê²½ê³ ", "ê¸ˆê¸°", "ì£¼ì˜", "ì´ìƒë°˜ì‘", "ë¶€ì‘ìš©", "ë¶€ì •ë°˜ì‘", "ì•Œë ˆë¥´ê¸°", "ê³¼ë¯¼ë°˜ì‘", "ì¤‘ë…", "ì¤‘ë…ì„±", "ì˜ì¡´ì„±", "ìŠµê´€ì„±", "ë‚´ì„±", "ë‚´ì•½ì„±", "ë‚´ì„±ë°œìƒ", "ë‚´ì•½ì„±ë°œìƒ", "ì£¼ì˜í™˜ì", "ì£¼ì˜í•„ìš”", "ì£¼ì˜ëŒ€ìƒ"],
            "weight": 1.0
        },
        "ìƒí˜¸ì‘ìš©": {
            "keywords": ["ìƒí˜¸ì‘ìš©", "ì•½ë¬¼ìƒí˜¸ì‘ìš©", "ì•½ë¬¼ê°„ìƒí˜¸ì‘ìš©", "ì•½ë¬¼ì¡°í•©", "ì•½ë¬¼ë³‘ìš©", "ì•½ë¬¼ë°°í•©", "ì•½ë¬¼ê²°í•©", "ì•½ë¬¼ë°˜ì‘", "ì•½ë¬¼íš¨ê³¼", "ì•½ë¬¼ì˜í–¥", "ì•½ë¬¼ì‘ìš©", "ì•½ë¬¼ë°˜ì‘ì„±", "ì•½ë¬¼ë¯¼ê°ì„±"],
            "weight": 0.9
        },
        "ì„ë¶€ ë° ìˆ˜ìœ ë¶€ ì‚¬ìš©": {
            "keywords": ["ì„ë¶€", "ì„ì‹ ë¶€", "ì„ì‹ ", "ìˆ˜ìœ ë¶€", "ìˆ˜ìœ ", "íƒœì•„", "íƒœì•„ê¸°", "íƒœì•„ë°œë‹¬", "íƒœì•„ì˜í–¥", "íƒœì•„ë…ì„±", "íƒœì•„ê¸°í˜•", "íƒœì•„ê¸°í˜•ì„±", "íƒœì•„ë°œë‹¬ì¥ì• ", "íƒœì•„ë°œë‹¬ì§€ì—°", "íƒœì•„ë°œë‹¬ì˜í–¥"],
            "weight": 0.9
        },
        "ê³ ë ¹ì ì‚¬ìš©": {
            "keywords": ["ê³ ë ¹ì", "ë…¸ì¸", "ë…¸ë…„", "ê³ ë ¹", "ë…¸í™”", "ë…¸ì¸ì„±", "ë…¸ë…„ì„±", "ê³ ë ¹ììš©", "ë…¸ì¸ìš©", "ë…¸ë…„ìš©", "ê³ ë ¹ìì í•©", "ë…¸ì¸ì í•©", "ë…¸ë…„ì í•©"],
            "weight": 0.8
        },
        "ì ìš© ì‹œ ì£¼ì˜ì‚¬í•­": {
            "keywords": ["ì ìš©", "ì ìš©ì‹œ", "ì ìš©ì‹œì£¼ì˜", "ì ìš©ì£¼ì˜", "ì ìš©ì‹œì£¼ì˜ì‚¬í•­", "ì ìš©ì£¼ì˜ì‚¬í•­", "ì ìš©ì‹œì£¼ì˜ì ", "ì ìš©ì£¼ì˜ì ", "ì ìš©ì‹œì£¼ì˜í• ì ", "ì ìš©ì£¼ì˜í• ì "],
            "weight": 0.8
        },
        "ë³´ê´€ ë° ì·¨ê¸‰": {
            "keywords": ["ë³´ê´€", "ë³´ê´€ì¡°ê±´", "ë³´ê´€ë°©ë²•", "ë³´ê´€ì˜¨ë„", "ë³´ê´€ìŠµë„", "ë³´ê´€ì¥ì†Œ", "ë³´ê´€ê¸°ê°„", "ë³´ê´€ì£¼ì˜", "ë³´ê´€ì£¼ì˜ì‚¬í•­", "ë³´ê´€ì£¼ì˜ì ", "ë³´ê´€ì£¼ì˜í• ì ", "ì·¨ê¸‰", "ì·¨ê¸‰ë°©ë²•", "ì·¨ê¸‰ì£¼ì˜", "ì·¨ê¸‰ì£¼ì˜ì‚¬í•­", "ì·¨ê¸‰ì£¼ì˜ì ", "ì·¨ê¸‰ì£¼ì˜í• ì ", "í¬ì¥", "í¬ì¥ë‹¨ìœ„", "í¬ì¥ë°©ë²•", "í¬ì¥ì£¼ì˜", "í¬ì¥ì£¼ì˜ì‚¬í•­"],
            "weight": 1.0
        },
        "ì œì¡° ë° íŒë§¤ì‚¬ ì •ë³´": {
            "keywords": ["ì œì¡°ì‚¬", "ì œì¡°ì—…ì²´", "ì œì¡°íšŒì‚¬", "ì œì¡°ê¸°ì—…", "ì œì¡°ê³µì¥", "ì œì¡°ì‹œì„¤", "ì œì¡°ì„¤ë¹„", "ì œì¡°ë¼ì¸", "ì œì¡°ê³µì •", "ì œì¡°ê³¼ì •", "ì œì¡°ë°©ë²•", "ì œì¡°ê¸°ìˆ ", "ì œì¡°í’ˆì§ˆ", "ì œì¡°ê´€ë¦¬", "íŒë§¤ì‚¬", "íŒë§¤ì—…ì²´", "íŒë§¤íšŒì‚¬", "íŒë§¤ê¸°ì—…", "íŒë§¤ì ", "íŒë§¤ì†Œ", "íŒë§¤ì²˜", "íŒë§¤ë§", "íŒë§¤ë„¤íŠ¸ì›Œí¬", "ê³µì¥", "ê³µì¥ì£¼ì†Œ", "ê³µì¥ìœ„ì¹˜", "ê³µì¥ì†Œì¬ì§€", "ê³µì¥ì†Œì¬", "ì†Œë¹„ììƒë‹´ì‹¤", "ê³ ê°ìƒë‹´ì‹¤", "ìƒë‹´ì‹¤", "ìƒë‹´ì†Œ", "ìƒë‹´ì„¼í„°"],
            "weight": 0.7
        }
    }
    
    section_keywords = {
        "ì„±ë¶„ ë° í•¨ëŸ‰": [],
        "ì„±ìƒ": [],
        "íš¨ëŠ¥ ë° íš¨ê³¼": [],
        "ìš©ë²• ë° ìš©ëŸ‰": [],
        "ì‚¬ìš©ìƒ ì£¼ì˜ì‚¬í•­": [],
        "ìƒí˜¸ì‘ìš©": [],
        "ì„ë¶€ ë° ìˆ˜ìœ ë¶€ ì‚¬ìš©": [],
        "ê³ ë ¹ì ì‚¬ìš©": [],
        "ì ìš© ì‹œ ì£¼ì˜ì‚¬í•­": [],
        "ë³´ê´€ ë° ì·¨ê¸‰": [],
        "ì œì¡° ë° íŒë§¤ì‚¬ ì •ë³´": []
    }
    
    for keyword in keywords:
        keyword_lower = keyword.lower()
        best_section = None
        best_score = 0.0
        
        # ê° í•­ëª©ë³„ë¡œ ìœ ì‚¬ë„ ì ìˆ˜ ê³„ì‚°
        for section, config in section_keyword_sets.items():
            section_keywords_list = config["keywords"]
            weight = config["weight"]
            
            # Jaccard ìœ ì‚¬ë„ ê³„ì‚°
            keyword_tokens = set(keyword_lower.split())
            section_tokens = set()
            
            for section_keyword in section_keywords_list:
                section_tokens.update(section_keyword.lower().split())
            
            # ì§ì ‘ ë§¤ì¹­ ì ìˆ˜
            direct_match_score = 0.0
            for section_keyword in section_keywords_list:
                if section_keyword.lower() in keyword_lower or keyword_lower in section_keyword.lower():
                    direct_match_score += 1.0
            
            # ë¶€ë¶„ ë§¤ì¹­ ì ìˆ˜
            partial_match_score = 0.0
            for section_keyword in section_keywords_list:
                if any(token in keyword_lower for token in section_keyword.lower().split()):
                    partial_match_score += 0.5
            
            # ìˆ«ìê°€ í¬í•¨ëœ í‚¤ì›Œë“œëŠ” ì„±ë¶„ ì •ë³´ì— ë†’ì€ ì ìˆ˜
            number_bonus = 0.0
            if re.search(r'\d+', keyword) and section == "ì„±ë¶„ ì •ë³´":
                number_bonus = 0.3
            
            # ìµœì¢… ì ìˆ˜ ê³„ì‚°
            total_score = (direct_match_score + partial_match_score + number_bonus) * weight
            
            if total_score > best_score:
                best_score = total_score
                best_section = section
        
        # ìµœê³  ì ìˆ˜ í•­ëª©ì— í‚¤ì›Œë“œ ë°°ì • (ì ìˆ˜ê°€ 0.5 ì´ìƒì¸ ê²½ìš°ë§Œ)
        if best_section and best_score >= 0.5:
            section_keywords[best_section].append(keyword)
        else:
            # ì ìˆ˜ê°€ ë‚®ì€ ê²½ìš° ê¸°ë³¸ ë¶„ë¥˜ ê·œì¹™ ì ìš©
            if re.search(r'\d+', keyword):
                section_keywords["ì„±ë¶„ ë° í•¨ëŸ‰"].append(keyword)
            elif any(word in keyword_lower for word in ['ì •', 'ìº¡ìŠ', 'ì£¼ì‚¬ì œ', 'ì œí˜•']):
                section_keywords["ì„±ìƒ"].append(keyword)
            elif any(word in keyword_lower for word in ['ìƒ‰ìƒ', 'ëª¨ì–‘', 'ê°ì¸']):
                section_keywords["ì„±ìƒ"].append(keyword)
            elif any(word in keyword_lower for word in ['ìš©ê¸°', 'í¬ì¥']):
                section_keywords["ë³´ê´€ ë° ì·¨ê¸‰"].append(keyword)
            else:
                # ê¸°ë³¸ê°’ìœ¼ë¡œ ì„±ë¶„ ë° í•¨ëŸ‰ì— ë°°ì •
                section_keywords["ì„±ë¶„ ë° í•¨ëŸ‰"].append(keyword)
    
    return section_keywords

def calculate_keyword_weights(keywords: List[str], product_tokens: List[str], keyword_frequency: Dict[str, int]) -> List[Tuple[str, float]]:
    """í‚¤ì›Œë“œì— ê°€ì¤‘ì¹˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
    from difflib import SequenceMatcher
    
    keyword_weights = []
    
    for keyword in keywords:
        weight = 0.0
        
        # 1. ì¶œí˜„ ë¹ˆë„ ì ìˆ˜ (40%)
        frequency = keyword_frequency.get(keyword, 1)
        frequency_score = min(frequency * 0.4, 0.4)
        
        # 2. ì œí’ˆëª…ê³¼ì˜ ìœ ì‚¬ë„ ì ìˆ˜ (40%)
        similarity_score = 0.0
        for token in product_tokens:
            similarity = SequenceMatcher(None, keyword.lower(), token.lower()).ratio()
            similarity_score = max(similarity_score, similarity)
        similarity_score *= 0.4
        
        # 3. ë¬¸ë§¥ì  ê´€ë ¨ì„± ì ìˆ˜ (20%)
        context_score = 0.0
        if len(keyword) >= 1:  # 1ê¸€ì ì´ìƒì¸ í† í°ë„ í¬í•¨
            has_number = bool(re.search(r'\d', keyword))
            has_special = bool(re.search(r'[^\w\s]', keyword))
            has_korean = bool(re.search(r'[ê°€-í£]', keyword))
            has_english = bool(re.search(r'[a-zA-Z]', keyword))
            
            # í•œê¸€ì´ë‚˜ ì˜ë¬¸ì´ í¬í•¨ëœ í† í°ì— ë” ë†’ì€ ì ìˆ˜
            context_score = (0.3 + 0.2 * has_number + 0.2 * has_special + 0.2 * has_korean + 0.1 * has_english) * 0.2
        
        weight = frequency_score + similarity_score + context_score
        keyword_weights.append((keyword, weight))
    
    # ê°€ì¤‘ì¹˜ ìˆœìœ¼ë¡œ ì •ë ¬
    keyword_weights.sort(key=lambda x: x[1], reverse=True)
    return keyword_weights

def get_top_keywords_by_section(section_keywords: Dict[str, List[str]], product_tokens: List[str], keyword_frequency: Dict[str, int], top_n: int = 3) -> Dict[str, List[str]]:
    """ê° í•­ëª©ë³„ë¡œ ìƒìœ„ í‚¤ì›Œë“œë¥¼ ì„ ì •í•©ë‹ˆë‹¤."""
    top_keywords = {}
    
    for section, keywords in section_keywords.items():
        if not keywords:
            continue
        
        keyword_weights = calculate_keyword_weights(keywords, product_tokens, keyword_frequency)
        top_n_keywords = [kw for kw, weight in keyword_weights[:top_n]]
        top_keywords[section] = top_n_keywords
    
    return top_keywords

def generate_section_sentences(product_name: str, section_keywords: Dict[str, List[str]]) -> Dict[str, str]:
    """ê° í•­ëª©ë³„ë¡œ LLM í”„ë¡¬í”„íŠ¸ë¡œ ë¬¸ì¥ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    generated_sentences = {}
    
    # í•­ëª©ë³„ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ (í•œêµ­ ì˜ì•½í’ˆ ì„¤ëª…ì„œ êµ¬ì¡°ì— ë§ì¶¤)
    section_prompts = {
        "ì„±ë¶„ ë° í•¨ëŸ‰": """ë„ˆëŠ” ì˜ì•½í’ˆ ê°œìš”ì„œë¥¼ ì‘ì„±í•˜ëŠ” ì „ë¬¸ê°€ì•¼.
ì œí’ˆëª…ì€ "{product_name}"ì´ê³ , ì§€ê¸ˆì€ "ì„±ë¶„ ë° í•¨ëŸ‰" í•­ëª©ì„ ì‘ì„± ì¤‘ì´ì•¼.

ì´ í•­ëª©ì— ëŒ€í•œ í‚¤ì›Œë“œëŠ” ë‹¤ìŒê³¼ ê°™ì•„:
{keywords}

ë§Œì•½ ë¬¸ì„œì—ì„œ ì¶”ì¶œëœ í‚¤ì›Œë“œê°€ ë¶€ì¡±í•˜ê±°ë‚˜ ì—†ë‹¤ë©´,
**ì˜ì•½í’ˆ ìƒì‹ê³¼ ì œí’ˆëª… ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¼ë°˜ì ì¸ ë¬¸ì¥ì„ ìƒì„±**í•´ì¤˜.

ì˜ˆì‹œ:
- ì•„ìŠ¤í”¼ë¦°: "ì´ ì œí’ˆì€ ì•„ìŠ¤í”¼ë¦°ì„ ì£¼ì„±ë¶„ìœ¼ë¡œ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆë‹¤."
- íŒŒì„¸íƒ€ë¯¼: "ì´ ì œí’ˆì€ íŒŒì„¸íƒ€ë¯¼ì„ ì£¼ì„±ë¶„ìœ¼ë¡œ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆë‹¤."

ì‘ë‹µì€ ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œ, ì•„ë˜ êµ¬ì¡°ë¡œ ë°˜í™˜í•´:

{{
  "ì„±ë¶„ ë° í•¨ëŸ‰": "[í‚¤ì›Œë“œ ê¸°ë°˜ ë˜ëŠ” ì¼ë°˜ì ì¸ ì„±ë¶„ ë° í•¨ëŸ‰ ì„¤ëª…]"
}}""",

        "ì„±ìƒ": """ë„ˆëŠ” ì˜ì•½í’ˆ ê°œìš”ì„œë¥¼ ì‘ì„±í•˜ëŠ” ì „ë¬¸ê°€ì•¼.
ì œí’ˆëª…ì€ "{product_name}"ì´ê³ , ì§€ê¸ˆì€ "ì„±ìƒ" í•­ëª©ì„ ì‘ì„± ì¤‘ì´ì•¼.

ì´ í•­ëª©ì— ëŒ€í•œ í‚¤ì›Œë“œëŠ” ë‹¤ìŒê³¼ ê°™ì•„:
{keywords}

ë§Œì•½ ë¬¸ì„œì—ì„œ ì¶”ì¶œëœ í‚¤ì›Œë“œê°€ ë¶€ì¡±í•˜ê±°ë‚˜ ì—†ë‹¤ë©´,
**ì˜ì•½í’ˆ ìƒì‹ê³¼ ì œí’ˆëª… ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¼ë°˜ì ì¸ ë¬¸ì¥ì„ ìƒì„±**í•´ì¤˜.

ì˜ˆì‹œ:
- ì •ì œ: "ì´ ì œí’ˆì€ ì¼ë°˜ì ì¸ ì •ì œ í˜•íƒœë¡œ ì œì¡°ë˜ì—ˆìŠµë‹ˆë‹¤."
- ìº¡ìŠ: "ì´ ì œí’ˆì€ ì¼ë°˜ì ì¸ ìº¡ìŠ í˜•íƒœë¡œ ì œì¡°ë˜ì—ˆìŠµë‹ˆë‹¤."

ì‘ë‹µì€ ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œ, ì•„ë˜ êµ¬ì¡°ë¡œ ë°˜í™˜í•´:

{{
  "ì„±ìƒ": "[í‚¤ì›Œë“œ ê¸°ë°˜ ë˜ëŠ” ì¼ë°˜ì ì¸ ì„±ìƒ ì„¤ëª…]"
}}""",

        "íš¨ëŠ¥ ë° íš¨ê³¼": """ë„ˆëŠ” ì˜ì•½í’ˆ ê°œìš”ì„œë¥¼ ì‘ì„±í•˜ëŠ” ì „ë¬¸ê°€ì•¼.
ì œí’ˆëª…ì€ "{product_name}"ì´ê³ , ì§€ê¸ˆì€ "íš¨ëŠ¥ ë° íš¨ê³¼" í•­ëª©ì„ ì‘ì„± ì¤‘ì´ì•¼.

ì´ í•­ëª©ì— ëŒ€í•œ í‚¤ì›Œë“œëŠ” ë‹¤ìŒê³¼ ê°™ì•„:
{keywords}

ë§Œì•½ ë¬¸ì„œì—ì„œ ì¶”ì¶œëœ í‚¤ì›Œë“œê°€ ë¶€ì¡±í•˜ê±°ë‚˜ ì—†ë‹¤ë©´,
**ì˜ì•½í’ˆ ìƒì‹ê³¼ ì œí’ˆëª… ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¼ë°˜ì ì¸ ë¬¸ì¥ì„ ìƒì„±**í•´ì¤˜.

ì˜ˆì‹œ:
- ì•„ìŠ¤í”¼ë¦°: "ì´ ì œí’ˆì€ í•´ì—´, ì§„í†µ, ì†Œì—¼ ì‘ìš©ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤."
- íŒŒì„¸íƒ€ë¯¼: "ì´ ì œí’ˆì€ í•´ì—´ ë° ì§„í†µ íš¨ê³¼ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤."

ì‘ë‹µì€ ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œ, ì•„ë˜ êµ¬ì¡°ë¡œ ë°˜í™˜í•´:

{{
  "íš¨ëŠ¥ ë° íš¨ê³¼": "[í‚¤ì›Œë“œ ê¸°ë°˜ ë˜ëŠ” ì¼ë°˜ì ì¸ íš¨ëŠ¥ ë° íš¨ê³¼ ì„¤ëª…]"
}}""",

        "ìš©ë²• ë° ìš©ëŸ‰": """ë„ˆëŠ” ì˜ì•½í’ˆ ê°œìš”ì„œë¥¼ ì‘ì„±í•˜ëŠ” ì „ë¬¸ê°€ì•¼.
ì œí’ˆëª…ì€ "{product_name}"ì´ê³ , ì§€ê¸ˆì€ "ìš©ë²• ë° ìš©ëŸ‰" í•­ëª©ì„ ì‘ì„± ì¤‘ì´ì•¼.

ì´ í•­ëª©ì— ëŒ€í•œ í‚¤ì›Œë“œëŠ” ë‹¤ìŒê³¼ ê°™ì•„:
{keywords}

ë§Œì•½ ë¬¸ì„œì—ì„œ ì¶”ì¶œëœ í‚¤ì›Œë“œê°€ ë¶€ì¡±í•˜ê±°ë‚˜ ì—†ë‹¤ë©´,
**ì˜ì•½í’ˆ ìƒì‹ê³¼ ì œí’ˆëª… ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¼ë°˜ì ì¸ ë¬¸ì¥ì„ ìƒì„±**í•´ì¤˜.

ì˜ˆì‹œ:
- ì •ì œ: "ì„±ì¸ 1íšŒ 1ì •ì„ 1ì¼ 3íšŒ ì‹í›„ì— ë³µìš©í•©ë‹ˆë‹¤."
- ìº¡ìŠ: "ì„±ì¸ 1íšŒ 1ìº¡ìŠì„ 1ì¼ 2íšŒ ì‹í›„ì— ë³µìš©í•©ë‹ˆë‹¤."

ì‘ë‹µì€ ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œ, ì•„ë˜ êµ¬ì¡°ë¡œ ë°˜í™˜í•´:

{{
  "ìš©ë²• ë° ìš©ëŸ‰": "[í‚¤ì›Œë“œ ê¸°ë°˜ ë˜ëŠ” ì¼ë°˜ì ì¸ ìš©ë²• ë° ìš©ëŸ‰ ì„¤ëª…]"
}}""",

        "ì‚¬ìš©ìƒ ì£¼ì˜ì‚¬í•­": """ë„ˆëŠ” ì˜ì•½í’ˆ ê°œìš”ì„œë¥¼ ì‘ì„±í•˜ëŠ” ì „ë¬¸ê°€ì•¼.
ì œí’ˆëª…ì€ "{product_name}"ì´ê³ , ì§€ê¸ˆì€ "ì‚¬ìš©ìƒ ì£¼ì˜ì‚¬í•­" í•­ëª©ì„ ì‘ì„± ì¤‘ì´ì•¼.

ì´ í•­ëª©ì— ëŒ€í•œ í‚¤ì›Œë“œëŠ” ë‹¤ìŒê³¼ ê°™ì•„:
{keywords}

ë§Œì•½ ë¬¸ì„œì—ì„œ ì¶”ì¶œëœ í‚¤ì›Œë“œê°€ ë¶€ì¡±í•˜ê±°ë‚˜ ì—†ë‹¤ë©´,
**ì˜ì•½í’ˆ ìƒì‹ê³¼ ì œí’ˆëª… ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¼ë°˜ì ì¸ ë¬¸ì¥ì„ ìƒì„±**í•´ì¤˜.

ì˜ˆì‹œ:
- "ì´ ì œí’ˆì€ ì•Œë ˆë¥´ê¸° ë°˜ì‘ì´ ë‚˜íƒ€ë‚  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì£¼ì˜ê°€ í•„ìš”í•©ë‹ˆë‹¤."
- "ì´ ì œí’ˆì€ ìœ„ì¥ ì¥ì• ë¥¼ ì¼ìœ¼í‚¬ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì‹í›„ ë³µìš©ì„ ê¶Œì¥í•©ë‹ˆë‹¤."

ì‘ë‹µì€ ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œ, ì•„ë˜ êµ¬ì¡°ë¡œ ë°˜í™˜í•´:

{{
  "ì‚¬ìš©ìƒ ì£¼ì˜ì‚¬í•­": "[í‚¤ì›Œë“œ ê¸°ë°˜ ë˜ëŠ” ì¼ë°˜ì ì¸ ì‚¬ìš©ìƒ ì£¼ì˜ì‚¬í•­ ì„¤ëª…]"
}}""",

        "ìƒí˜¸ì‘ìš©": """ë„ˆëŠ” ì˜ì•½í’ˆ ê°œìš”ì„œë¥¼ ì‘ì„±í•˜ëŠ” ì „ë¬¸ê°€ì•¼.
ì œí’ˆëª…ì€ "{product_name}"ì´ê³ , ì§€ê¸ˆì€ "ìƒí˜¸ì‘ìš©" í•­ëª©ì„ ì‘ì„± ì¤‘ì´ì•¼.

ì´ í•­ëª©ì— ëŒ€í•œ í‚¤ì›Œë“œëŠ” ë‹¤ìŒê³¼ ê°™ì•„:
{keywords}

ë§Œì•½ ë¬¸ì„œì—ì„œ ì¶”ì¶œëœ í‚¤ì›Œë“œê°€ ë¶€ì¡±í•˜ê±°ë‚˜ ì—†ë‹¤ë©´,
**ì˜ì•½í’ˆ ìƒì‹ê³¼ ì œí’ˆëª… ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¼ë°˜ì ì¸ ë¬¸ì¥ì„ ìƒì„±**í•´ì¤˜.

ì˜ˆì‹œ:
- "ì´ ì œí’ˆì€ ë‹¤ë¥¸ ì•½ë¬¼ê³¼ ìƒí˜¸ì‘ìš©í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì˜ì‚¬ì™€ ìƒë‹´ í›„ ë³µìš©í•˜ì„¸ìš”."
- "ì´ ì œí’ˆì€ í•­ì‘ê³ ì œì™€ í•¨ê»˜ ë³µìš© ì‹œ ì¶œí˜ˆ ìœ„í—˜ì´ ì¦ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."

ì‘ë‹µì€ ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œ, ì•„ë˜ êµ¬ì¡°ë¡œ ë°˜í™˜í•´:

{{
  "ìƒí˜¸ì‘ìš©": "[í‚¤ì›Œë“œ ê¸°ë°˜ ë˜ëŠ” ì¼ë°˜ì ì¸ ìƒí˜¸ì‘ìš© ì„¤ëª…]"
}}""",

        "ì„ë¶€ ë° ìˆ˜ìœ ë¶€ ì‚¬ìš©": """ë„ˆëŠ” ì˜ì•½í’ˆ ê°œìš”ì„œë¥¼ ì‘ì„±í•˜ëŠ” ì „ë¬¸ê°€ì•¼.
ì œí’ˆëª…ì€ "{product_name}"ì´ê³ , ì§€ê¸ˆì€ "ì„ë¶€ ë° ìˆ˜ìœ ë¶€ ì‚¬ìš©" í•­ëª©ì„ ì‘ì„± ì¤‘ì´ì•¼.

ì´ í•­ëª©ì— ëŒ€í•œ í‚¤ì›Œë“œëŠ” ë‹¤ìŒê³¼ ê°™ì•„:
{keywords}

ë§Œì•½ ë¬¸ì„œì—ì„œ ì¶”ì¶œëœ í‚¤ì›Œë“œê°€ ë¶€ì¡±í•˜ê±°ë‚˜ ì—†ë‹¤ë©´,
**ì˜ì•½í’ˆ ìƒì‹ê³¼ ì œí’ˆëª… ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¼ë°˜ì ì¸ ë¬¸ì¥ì„ ìƒì„±**í•´ì¤˜.

ì˜ˆì‹œ:
- "ì„ì‹  ì¤‘ì—ëŠ” ì˜ì‚¬ì™€ ìƒë‹´ í›„ ë³µìš©í•˜ì„¸ìš”."
- "ìˆ˜ìœ  ì¤‘ì—ëŠ” ì˜ì‚¬ì™€ ìƒë‹´ í›„ ë³µìš©í•˜ì„¸ìš”."

ì‘ë‹µì€ ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œ, ì•„ë˜ êµ¬ì¡°ë¡œ ë°˜í™˜í•´:

{{
  "ì„ë¶€ ë° ìˆ˜ìœ ë¶€ ì‚¬ìš©": "[í‚¤ì›Œë“œ ê¸°ë°˜ ë˜ëŠ” ì¼ë°˜ì ì¸ ì„ë¶€ ë° ìˆ˜ìœ ë¶€ ì‚¬ìš© ì„¤ëª…]"
}}""",

        "ê³ ë ¹ì ì‚¬ìš©": """ë„ˆëŠ” ì˜ì•½í’ˆ ê°œìš”ì„œë¥¼ ì‘ì„±í•˜ëŠ” ì „ë¬¸ê°€ì•¼.
ì œí’ˆëª…ì€ "{product_name}"ì´ê³ , ì§€ê¸ˆì€ "ê³ ë ¹ì ì‚¬ìš©" í•­ëª©ì„ ì‘ì„± ì¤‘ì´ì•¼.

ì´ í•­ëª©ì— ëŒ€í•œ í‚¤ì›Œë“œëŠ” ë‹¤ìŒê³¼ ê°™ì•„:
{keywords}

ë§Œì•½ ë¬¸ì„œì—ì„œ ì¶”ì¶œëœ í‚¤ì›Œë“œê°€ ë¶€ì¡±í•˜ê±°ë‚˜ ì—†ë‹¤ë©´,
**ì˜ì•½í’ˆ ìƒì‹ê³¼ ì œí’ˆëª… ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¼ë°˜ì ì¸ ë¬¸ì¥ì„ ìƒì„±**í•´ì¤˜.

ì˜ˆì‹œ:
- "ê³ ë ¹ìëŠ” ì‹ ì¥ ê¸°ëŠ¥ ì €í•˜ë¡œ ì¸í•´ ìš©ëŸ‰ ì¡°ì •ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
- "ê³ ë ¹ìëŠ” ì˜ì‚¬ì™€ ìƒë‹´ í›„ ë³µìš©í•˜ì„¸ìš”."

ì‘ë‹µì€ ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œ, ì•„ë˜ êµ¬ì¡°ë¡œ ë°˜í™˜í•´:

{{
  "ê³ ë ¹ì ì‚¬ìš©": "[í‚¤ì›Œë“œ ê¸°ë°˜ ë˜ëŠ” ì¼ë°˜ì ì¸ ê³ ë ¹ì ì‚¬ìš© ì„¤ëª…]"
}}""",

        "ì ìš© ì‹œ ì£¼ì˜ì‚¬í•­": """ë„ˆëŠ” ì˜ì•½í’ˆ ê°œìš”ì„œë¥¼ ì‘ì„±í•˜ëŠ” ì „ë¬¸ê°€ì•¼.
ì œí’ˆëª…ì€ "{product_name}"ì´ê³ , ì§€ê¸ˆì€ "ì ìš© ì‹œ ì£¼ì˜ì‚¬í•­" í•­ëª©ì„ ì‘ì„± ì¤‘ì´ì•¼.

ì´ í•­ëª©ì— ëŒ€í•œ í‚¤ì›Œë“œëŠ” ë‹¤ìŒê³¼ ê°™ì•„:
{keywords}

ë§Œì•½ ë¬¸ì„œì—ì„œ ì¶”ì¶œëœ í‚¤ì›Œë“œê°€ ë¶€ì¡±í•˜ê±°ë‚˜ ì—†ë‹¤ë©´,
**ì˜ì•½í’ˆ ìƒì‹ê³¼ ì œí’ˆëª… ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¼ë°˜ì ì¸ ë¬¸ì¥ì„ ìƒì„±**í•´ì¤˜.

ì˜ˆì‹œ:
- "ì´ ì œí’ˆì€ ì°¨ëŸ‰ ìš´ì „ ì‹œ ì£¼ì˜ê°€ í•„ìš”í•©ë‹ˆë‹¤."
- "ì´ ì œí’ˆì€ ê¸°ê³„ ì¡°ì‘ ì‹œ ì£¼ì˜ê°€ í•„ìš”í•©ë‹ˆë‹¤."

ì‘ë‹µì€ ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œ, ì•„ë˜ êµ¬ì¡°ë¡œ ë°˜í™˜í•´:

{{
  "ì ìš© ì‹œ ì£¼ì˜ì‚¬í•­": "[í‚¤ì›Œë“œ ê¸°ë°˜ ë˜ëŠ” ì¼ë°˜ì ì¸ ì ìš© ì‹œ ì£¼ì˜ì‚¬í•­ ì„¤ëª…]"
}}""",

        "ë³´ê´€ ë° ì·¨ê¸‰": """ë„ˆëŠ” ì˜ì•½í’ˆ ê°œìš”ì„œë¥¼ ì‘ì„±í•˜ëŠ” ì „ë¬¸ê°€ì•¼.
ì œí’ˆëª…ì€ "{product_name}"ì´ê³ , ì§€ê¸ˆì€ "ë³´ê´€ ë° ì·¨ê¸‰" í•­ëª©ì„ ì‘ì„± ì¤‘ì´ì•¼.

ì´ í•­ëª©ì— ëŒ€í•œ í‚¤ì›Œë“œëŠ” ë‹¤ìŒê³¼ ê°™ì•„:
{keywords}

ë§Œì•½ ë¬¸ì„œì—ì„œ ì¶”ì¶œëœ í‚¤ì›Œë“œê°€ ë¶€ì¡±í•˜ê±°ë‚˜ ì—†ë‹¤ë©´,
**ì˜ì•½í’ˆ ìƒì‹ê³¼ ì œí’ˆëª… ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¼ë°˜ì ì¸ ë¬¸ì¥ì„ ìƒì„±**í•´ì¤˜.

ì˜ˆì‹œ:
- "ì´ ì œí’ˆì€ ì„œëŠ˜í•˜ê³  ê±´ì¡°í•œ ê³³ì— ë³´ê´€í•˜ì„¸ìš”."
- "ì´ ì œí’ˆì€ ì§ì‚¬ê´‘ì„ ì„ í”¼í•´ ë³´ê´€í•˜ì„¸ìš”."

ì‘ë‹µì€ ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œ, ì•„ë˜ êµ¬ì¡°ë¡œ ë°˜í™˜í•´:

{{
  "ë³´ê´€ ë° ì·¨ê¸‰": "[í‚¤ì›Œë“œ ê¸°ë°˜ ë˜ëŠ” ì¼ë°˜ì ì¸ ë³´ê´€ ë° ì·¨ê¸‰ ì„¤ëª…]"
}}""",

        "ì œì¡° ë° íŒë§¤ì‚¬ ì •ë³´": """ë„ˆëŠ” ì˜ì•½í’ˆ ê°œìš”ì„œë¥¼ ì‘ì„±í•˜ëŠ” ì „ë¬¸ê°€ì•¼.
ì œí’ˆëª…ì€ "{product_name}"ì´ê³ , ì§€ê¸ˆì€ "ì œì¡° ë° íŒë§¤ì‚¬ ì •ë³´" í•­ëª©ì„ ì‘ì„± ì¤‘ì´ì•¼.

ì´ í•­ëª©ì— ëŒ€í•œ í‚¤ì›Œë“œëŠ” ë‹¤ìŒê³¼ ê°™ì•„:
{keywords}

ë§Œì•½ ë¬¸ì„œì—ì„œ ì¶”ì¶œëœ í‚¤ì›Œë“œê°€ ë¶€ì¡±í•˜ê±°ë‚˜ ì—†ë‹¤ë©´,
**ì˜ì•½í’ˆ ìƒì‹ê³¼ ì œí’ˆëª… ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¼ë°˜ì ì¸ ë¬¸ì¥ì„ ìƒì„±**í•´ì¤˜.

ì˜ˆì‹œ:
- "ì´ ì œí’ˆì€ [ì œì¡°ì‚¬ëª…]ì—ì„œ ì œì¡°í•˜ê³  [íŒë§¤ì‚¬ëª…]ì—ì„œ íŒë§¤í•©ë‹ˆë‹¤."
- "ì´ ì œí’ˆì˜ ì œì¡°ì‚¬ëŠ” [ì œì¡°ì‚¬ëª…]ì…ë‹ˆë‹¤."

ì‘ë‹µì€ ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œ, ì•„ë˜ êµ¬ì¡°ë¡œ ë°˜í™˜í•´:

{{
  "ì œì¡° ë° íŒë§¤ì‚¬ ì •ë³´": "[í‚¤ì›Œë“œ ê¸°ë°˜ ë˜ëŠ” ì¼ë°˜ì ì¸ ì œì¡° ë° íŒë§¤ì‚¬ ì •ë³´ ì„¤ëª…]"
}}"""
    }
    
    for section, keywords in section_keywords.items():
        try:
            if keywords:
                # í‚¤ì›Œë“œê°€ ìˆëŠ” ê²½ìš°: í‚¤ì›Œë“œ ê¸°ë°˜ ë¬¸ì¥ ìƒì„±
                prompt_template = section_prompts.get(section, "")
                if prompt_template:
                    prompt = prompt_template.format(
                        product_name=product_name or "ì •ë³´ ì—†ìŒ",
                        keywords=str(keywords)
                    )
                    
                    print(f"ğŸ” {section} í•­ëª© ì²˜ë¦¬ ì¤‘...")
                    print(f"   ì œí’ˆëª…: {product_name}")
                    print(f"   í‚¤ì›Œë“œ: {keywords}")
                    
                    # LLM í˜¸ì¶œ
                    result = generate_overview_with_llm(prompt, "")
                    print(f"   LLM ì‘ë‹µ: {result}")
                    
                    # ê²°ê³¼ íŒŒì‹±
                    if isinstance(result, dict) and result:
                        # JSON ì‘ë‹µì¸ ê²½ìš°
                        if section in result:
                            generated_sentences[section] = result[section]
                            print(f"   âœ… {section} ë¬¸ì¥ ìƒì„± ì„±ê³µ: {result[section]}")
                        else:
                            # ë‹¤ë¥¸ í˜•íƒœì˜ JSON ì‘ë‹µ ì²˜ë¦¬
                            for key, value in result.items():
                                if isinstance(value, str):
                                    generated_sentences[section] = value
                                    print(f"   âœ… {section} ë¬¸ì¥ ìƒì„± ì„±ê³µ (ëŒ€ì²´ í‚¤): {value}")
                                    break
                            else:
                                # í‚¤ì›Œë“œë¡œ ì§ì ‘ ë¬¸ì¥ ìƒì„±
                                keywords_str = ", ".join(keywords)
                                generated_sentences[section] = f"{product_name}ì˜ {section}ëŠ” {keywords_str}ë¥¼ í¬í•¨í•©ë‹ˆë‹¤."
                                print(f"   âœ… {section} í‚¤ì›Œë“œ ê¸°ë°˜ ë¬¸ì¥ ìƒì„±")
                    elif isinstance(result, str) and result.strip():
                        # ë¬¸ìì—´ ì‘ë‹µì¸ ê²½ìš° JSON íŒŒì‹± ì‹œë„
                        try:
                            json_start = result.find('{')
                            json_end = result.rfind('}')
                            if json_start != -1 and json_end != -1:
                                json_str = result[json_start:json_end+1]
                                parsed = json.loads(json_str)
                                if section in parsed:
                                    generated_sentences[section] = parsed[section]
                                    print(f"   âœ… {section} ë¬¸ì¥ ìƒì„± ì„±ê³µ (JSON ì¶”ì¶œ): {parsed[section]}")
                                else:
                                    # í‚¤ì›Œë“œë¡œ ì§ì ‘ ë¬¸ì¥ ìƒì„±
                                    keywords_str = ", ".join(keywords)
                                    generated_sentences[section] = f"{product_name}ì˜ {section}ëŠ” {keywords_str}ë¥¼ í¬í•¨í•©ë‹ˆë‹¤."
                                    print(f"   âœ… {section} í‚¤ì›Œë“œ ê¸°ë°˜ ë¬¸ì¥ ìƒì„±")
                            else:
                                # í‚¤ì›Œë“œë¡œ ì§ì ‘ ë¬¸ì¥ ìƒì„±
                                keywords_str = ", ".join(keywords)
                                generated_sentences[section] = f"{product_name}ì˜ {section}ëŠ” {keywords_str}ë¥¼ í¬í•¨í•©ë‹ˆë‹¤."
                                print(f"   âœ… {section} í‚¤ì›Œë“œ ê¸°ë°˜ ë¬¸ì¥ ìƒì„±")
                        except json.JSONDecodeError:
                            # í‚¤ì›Œë“œë¡œ ì§ì ‘ ë¬¸ì¥ ìƒì„±
                            keywords_str = ", ".join(keywords)
                            generated_sentences[section] = f"{product_name}ì˜ {section}ëŠ” {keywords_str}ë¥¼ í¬í•¨í•©ë‹ˆë‹¤."
                            print(f"   âœ… {section} í‚¤ì›Œë“œ ê¸°ë°˜ ë¬¸ì¥ ìƒì„±")
                    else:
                        # ë¹ˆ ì‘ë‹µì¸ ê²½ìš° í‚¤ì›Œë“œë¡œ ì§ì ‘ ë¬¸ì¥ ìƒì„±
                        keywords_str = ", ".join(keywords)
                        generated_sentences[section] = f"{product_name}ì˜ {section}ëŠ” {keywords_str}ë¥¼ í¬í•¨í•©ë‹ˆë‹¤."
                        print(f"   âœ… {section} í‚¤ì›Œë“œ ê¸°ë°˜ ë¬¸ì¥ ìƒì„±")
                else:
                    # í‚¤ì›Œë“œë¡œ ì§ì ‘ ë¬¸ì¥ ìƒì„±
                    keywords_str = ", ".join(keywords)
                    generated_sentences[section] = f"{product_name}ì˜ {section}ëŠ” {keywords_str}ë¥¼ í¬í•¨í•©ë‹ˆë‹¤."
                    print(f"   âœ… {section} í‚¤ì›Œë“œ ê¸°ë°˜ ë¬¸ì¥ ìƒì„±")
            else:
                # í‚¤ì›Œë“œê°€ ì—†ëŠ” ê²½ìš°: LLMì—ê²Œ í•´ë‹¹ í•­ëª©ì— ë§ëŠ” ë¬¸ì¥ ìƒì„± ìš”ì²­
                no_keyword_prompt = f"""ë„ˆëŠ” ì‹ì•½ì²˜ CTD ê¸°ë°˜ ì˜ì•½í’ˆ ê°œìš”ì„œë¥¼ ì‘ì„±í•˜ëŠ” AIì•¼.
ì œí’ˆëª…ì€ "{product_name}"ì´ê³ , ì§€ê¸ˆì€ "{section}" í•­ëª©ì„ ì‘ì„± ì¤‘ì´ì•¼.
ì´ í•­ëª©ì— ëŒ€í•œ ì •ë³´ê°€ ë¬¸ì„œì—ì„œ ì¶”ì¶œë˜ì§€ ì•Šì•˜ì–´.

ì´ ì œí’ˆì˜ {section}ì— ëŒ€í•´ ì¼ë°˜ì ì¸ ì˜ì•½í’ˆ ê°œìš”ì„œ í˜•ì‹ìœ¼ë¡œ ë¬¸ì¥ì„ ì‘ì„±í•´ì¤˜.
ì‘ë‹µ í˜•ì‹ì€ ë°˜ë“œì‹œ JSONìœ¼ë¡œ, ì•„ë˜ êµ¬ì¡°ë¥¼ ë”°ë¥´ë„ë¡ í•´:

{{
  "{section}": "[í•´ë‹¹ í•­ëª©ì— ë§ëŠ” ì¼ë°˜ì ì¸ ì„¤ëª… ë¬¸ì¥]"
}}"""
                
                print(f"ğŸ” {section} í•­ëª© ì²˜ë¦¬ ì¤‘... (ì •ë³´ ì—†ìŒ)")
                print(f"   ì œí’ˆëª…: {product_name}")
                print(f"   í‚¤ì›Œë“œ: ì—†ìŒ - LLMì—ê²Œ ë¬¸ì¥ ìƒì„± ìš”ì²­")
                
                # LLM í˜¸ì¶œ
                result = generate_overview_with_llm(no_keyword_prompt, "")
                print(f"   LLM ì‘ë‹µ: {result}")
                
                # ê²°ê³¼ íŒŒì‹±
                if isinstance(result, dict) and result:
                    if section in result:
                        generated_sentences[section] = result[section]
                        print(f"   âœ… {section} LLM ë¬¸ì¥ ìƒì„± ì„±ê³µ: {result[section]}")
                    else:
                        # ê¸°ë³¸ ë¬¸ì¥ ìƒì„±
                        generated_sentences[section] = f"{product_name}ì˜ {section}ì— ëŒ€í•œ ì •ë³´ê°€ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
                        print(f"   âš ï¸ {section} ê¸°ë³¸ ë¬¸ì¥ ìƒì„±")
                elif isinstance(result, str) and result.strip():
                    try:
                        json_start = result.find('{')
                        json_end = result.rfind('}')
                        if json_start != -1 and json_end != -1:
                            json_str = result[json_start:json_end+1]
                            parsed = json.loads(json_str)
                            if section in parsed:
                                generated_sentences[section] = parsed[section]
                                print(f"   âœ… {section} LLM ë¬¸ì¥ ìƒì„± ì„±ê³µ: {parsed[section]}")
                            else:
                                generated_sentences[section] = f"{product_name}ì˜ {section}ì— ëŒ€í•œ ì •ë³´ê°€ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
                                print(f"   âš ï¸ {section} ê¸°ë³¸ ë¬¸ì¥ ìƒì„±")
                        else:
                            generated_sentences[section] = f"{product_name}ì˜ {section}ì— ëŒ€í•œ ì •ë³´ê°€ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
                            print(f"   âš ï¸ {section} ê¸°ë³¸ ë¬¸ì¥ ìƒì„±")
                    except json.JSONDecodeError:
                        generated_sentences[section] = f"{product_name}ì˜ {section}ì— ëŒ€í•œ ì •ë³´ê°€ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
                        print(f"   âš ï¸ {section} ê¸°ë³¸ ë¬¸ì¥ ìƒì„±")
                else:
                    generated_sentences[section] = f"{product_name}ì˜ {section}ì— ëŒ€í•œ ì •ë³´ê°€ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
                    print(f"   âš ï¸ {section} ê¸°ë³¸ ë¬¸ì¥ ìƒì„±")
            
        except Exception as e:
            print(f"âŒ ë¬¸ì¥ ìƒì„± ì˜¤ë¥˜ ({section}): {e}")
            if keywords:
                # í‚¤ì›Œë“œê°€ ìˆëŠ” ê²½ìš° í‚¤ì›Œë“œ ê¸°ë°˜ ë¬¸ì¥ ìƒì„±
                keywords_str = ", ".join(keywords)
                generated_sentences[section] = f"{product_name}ì˜ {section}ëŠ” {keywords_str}ë¥¼ í¬í•¨í•©ë‹ˆë‹¤."
            else:
                # í‚¤ì›Œë“œê°€ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ ë¬¸ì¥
                generated_sentences[section] = f"{product_name}ì˜ {section}ì— ëŒ€í•œ ì •ë³´ê°€ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
    
    return generated_sentences

def extract_medical_data_from_text(text: str, user_product_name: str = "") -> Dict[str, Any]:
    """í…ìŠ¤íŠ¸ì—ì„œ ì˜ì•½í’ˆ ê´€ë ¨ ë°ì´í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤ (í† í° ë‹¨ìœ„ í‚¤ì›Œë“œ ìˆ˜ì§‘ êµ¬ì¡°)."""
    from utils.keyword_processor import tokenize_product_name
    
    # Ollama ì—°ê²° ìƒíƒœ í™•ì¸
    if not test_ollama_connection():
        print("âš ï¸ Ollamaì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Ollama ì„œë¹„ìŠ¤ë¥¼ ì‹œì‘í•´ì£¼ì„¸ìš”.")
        return {"3.2.P.1": {}, "3.2.P.2": {}}
    
    # ì œí’ˆëª…ì´ ì—†ìœ¼ë©´ ì˜¤ë¥˜ ë°˜í™˜
    if not user_product_name:
        print("âŒ ì œí’ˆëª…ì´ ì…ë ¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return {"3.2.P.1": {}, "3.2.P.2": {}}
    
    product_name = user_product_name
    print(f"ì‚¬ìš©ì ì…ë ¥ ì œí’ˆëª…: {product_name}")
    
    # 1ë‹¨ê³„: ì œí’ˆëª… í† í°í™”
    print("1ë‹¨ê³„: ì œí’ˆëª… í† í°í™” ì¤‘...")
    product_tokens = tokenize_product_name(product_name)
    print(f"ì œí’ˆëª… í† í°: {product_tokens}")
    
    # 2ë‹¨ê³„: í…ìŠ¤íŠ¸ë¥¼ í† í° ë‹¨ìœ„ë¡œ ë¶„í• 
    print("2ë‹¨ê³„: í…ìŠ¤íŠ¸ë¥¼ í† í° ë‹¨ìœ„ë¡œ ë¶„í•  ì¤‘...")
    tokens = split_into_tokens(text)
    print(f"ì´ {len(tokens)}ê°œì˜ í† í°ìœ¼ë¡œ ë¶„í• ë¨")
    
    # 3ë‹¨ê³„: í† í°ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
    print("3ë‹¨ê³„: í† í°ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘...")
    all_keywords = extract_keywords_from_tokens(tokens)
    print(f"ì¶”ì¶œëœ ì´ í‚¤ì›Œë“œ ìˆ˜: {len(all_keywords)}")
    
    # 4ë‹¨ê³„: í‚¤ì›Œë“œ ì¶œí˜„ ë¹ˆë„ ê³„ì‚°
    print("4ë‹¨ê³„: í‚¤ì›Œë“œ ì¶œí˜„ ë¹ˆë„ ê³„ì‚° ì¤‘...")
    keyword_frequency = Counter(all_keywords)
    
    # 5ë‹¨ê³„: í‚¤ì›Œë“œë¥¼ í•­ëª©ë³„ë¡œ ë¶„ë¥˜
    print("5ë‹¨ê³„: í‚¤ì›Œë“œ í•­ëª©ë³„ ë¶„ë¥˜ ì¤‘...")
    section_keywords = classify_keywords_by_section(all_keywords)
    
    # 6ë‹¨ê³„: ê° í•­ëª©ë³„ ìƒìœ„ 3ê°œ í‚¤ì›Œë“œ ì„ íƒ
    print("6ë‹¨ê³„: ê° í•­ëª©ë³„ ìƒìœ„ í‚¤ì›Œë“œ ì„ íƒ ì¤‘...")
    top_keywords_by_section = get_top_keywords_by_section(
        section_keywords, product_tokens, keyword_frequency, top_n=3
    )
    
    # 7ë‹¨ê³„: ê° í•­ëª©ë³„ ë¬¸ì¥ ìƒì„±
    print("7ë‹¨ê³„: í•­ëª©ë³„ ë¬¸ì¥ ìƒì„± ì¤‘...")
    generated_sentences = generate_section_sentences(product_name, top_keywords_by_section)
    
    # 8ë‹¨ê³„: í•œêµ­ ì˜ì•½í’ˆ ì„¤ëª…ì„œ êµ¬ì¡°ë¡œ ë³€í™˜
    print("8ë‹¨ê³„: í•œêµ­ ì˜ì•½í’ˆ ì„¤ëª…ì„œ êµ¬ì¡°ë¡œ ë³€í™˜ ì¤‘...")
    final_data = create_korean_medicine_structure(product_name, generated_sentences)
    
    # ë””ë²„ê¹… ì •ë³´ ì €ì¥ (Streamlit ì„¸ì…˜ì— ì €ì¥)
    try:
        import streamlit as st
        if 'st' in globals():
            st.session_state['debug_keywords'] = {
                'all_keywords': all_keywords,
                'product_tokens': product_tokens,
                'section_keywords': section_keywords,
                'top_keywords_by_section': top_keywords_by_section,
                'keyword_frequency': dict(keyword_frequency),
                'token_info': {
                    'total_tokens': len(tokens),
                    'processing_method': 'í† í° ë‹¨ìœ„ í‚¤ì›Œë“œ ìˆ˜ì§‘'
                }
            }
            st.session_state['debug_sentences'] = generated_sentences
    except ImportError:
        # Streamlitì´ ì—†ëŠ” í™˜ê²½ì—ì„œëŠ” ë””ë²„ê¹… ì •ë³´ë¥¼ ì¶œë ¥ë§Œ
        print("ë””ë²„ê¹… ì •ë³´:")
        print(f"ì „ì²´ í‚¤ì›Œë“œ: {all_keywords}")
        print(f"ì œí’ˆëª… í† í°: {product_tokens}")
        print(f"í•­ëª©ë³„ í‚¤ì›Œë“œ: {section_keywords}")
        print(f"ìƒìœ„ í‚¤ì›Œë“œ: {top_keywords_by_section}")
        print(f"ìƒì„±ëœ ë¬¸ì¥: {generated_sentences}")
        print(f"í† í° ì •ë³´: {len(tokens)}ê°œ í† í°")
    
    print(f"ìµœì¢… ê²°ê³¼: {final_data}")
    return final_data

def clean_and_improve_text_with_ollama(product_name: str, original_text: str, section_name: str, subsection_name: str = None) -> str:
    """Ollamaë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ì •ì œí•˜ê³  ê°œì„ í•©ë‹ˆë‹¤. (ìµœì í™”ëœ ë²„ì „)"""
    try:
        # ê¸°ë³¸ í•œêµ­ì–´ ì •ì œ (ì™¸êµ­ì–´ ì œê±°)
        cleaned_text = clean_foreign_languages(original_text)
        
        # í•œêµ­ì–´ê°€ ì•„ë‹Œ ê²½ìš° ê¸°ë³¸ ë¬¸ì¥ ë°˜í™˜
        if not is_korean_text(cleaned_text) and not is_korean_text(original_text):
            return f"{product_name}ì˜ {section_name}{' - ' + subsection_name if subsection_name else ''}ì— ëŒ€í•œ ì •ë³´ì…ë‹ˆë‹¤."
        
        # í…ìŠ¤íŠ¸ê°€ ì´ë¯¸ ê¹¨ë—í•˜ê³  í•œêµ­ì–´ì¸ ê²½ìš° ê·¸ëŒ€ë¡œ ë°˜í™˜
        if is_korean_text(cleaned_text) and len(cleaned_text.strip()) > 10:
            return cleaned_text
        
        # Ollama í˜¸ì¶œì´ í•„ìš”í•œ ê²½ìš°ì—ë§Œ ì‹¤í–‰
        from llm.ollama_client import OllamaClient
        client = OllamaClient()
        
        # ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸ë¡œ ì†ë„ í–¥ìƒ
        if subsection_name:
            prompt = f"""ì œí’ˆëª…: {product_name}, í•­ëª©: {section_name}-{subsection_name}
ì›ë³¸: {cleaned_text}
ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë¡œ 1-2ë¬¸ì¥ìœ¼ë¡œ ì •ì œí•´ì£¼ì„¸ìš”. JSON í˜•ì‹: {{"{subsection_name}": "ë‚´ìš©"}}"""
        else:
            prompt = f"""ì œí’ˆëª…: {product_name}, í•­ëª©: {section_name}
ì›ë³¸: {cleaned_text}
ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë¡œ 1-2ë¬¸ì¥ìœ¼ë¡œ ì •ì œí•´ì£¼ì„¸ìš”. JSON í˜•ì‹: {{"{section_name}": "ë‚´ìš©"}}"""
        
        print(f"ğŸ”§ {section_name}{' - ' + subsection_name if subsection_name else ''} í•­ëª© í…ìŠ¤íŠ¸ ì •ì œ ì¤‘...")
        
        result = client.generate_response(prompt)
        
        if result.get("error", False):
            print(f"   âŒ Ollama ì˜¤ë¥˜: {result.get('text', 'Unknown error')}")
            return f"{product_name}ì˜ {section_name}{' - ' + subsection_name if subsection_name else ''}ì— ëŒ€í•œ ì •ë³´ì…ë‹ˆë‹¤."
        
        response_text = result.get("text", "")
        if not response_text:
            return f"{product_name}ì˜ {section_name}{' - ' + subsection_name if subsection_name else ''}ì— ëŒ€í•œ ì •ë³´ì…ë‹ˆë‹¤."
        
        # JSON ì¶”ì¶œ
        json_result = client.extract_json_from_response(response_text)
        
        if json_result:
            if subsection_name and subsection_name in json_result:
                content = json_result[subsection_name]
                print(f"   âœ… {subsection_name} ì •ì œ ì„±ê³µ: {content[:50]}...")
                return content
            elif section_name in json_result:
                content = json_result[section_name]
                print(f"   âœ… {section_name} ì •ì œ ì„±ê³µ: {content[:50]}...")
                return content
        
        # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ë¬¸ì¥ ë°˜í™˜
        return f"{product_name}ì˜ {section_name}{' - ' + subsection_name if subsection_name else ''}ì— ëŒ€í•œ ì •ë³´ì…ë‹ˆë‹¤."
        
    except Exception as e:
        print(f"âŒ Ollama í…ìŠ¤íŠ¸ ì •ì œ ì˜¤ë¥˜ ({section_name}): {e}")
        return f"{product_name}ì˜ {section_name}{' - ' + subsection_name if subsection_name else ''}ì— ëŒ€í•œ ì •ë³´ì…ë‹ˆë‹¤."

def generate_missing_content_with_ollama(product_name: str, section_name: str, subsection_name: str = None) -> str:
    """Ollamaë¥¼ ì‚¬ìš©í•˜ì—¬ ëˆ„ë½ëœ ë‚´ìš©ì„ ìƒì„±í•©ë‹ˆë‹¤. (ìµœì í™”ëœ ë²„ì „)"""
    try:
        from llm.ollama_client import OllamaClient
        
        client = OllamaClient()
        
        # ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸ë¡œ ì†ë„ í–¥ìƒ
        if subsection_name:
            prompt = f"""ì œí’ˆëª…: {product_name}, í•­ëª©: {section_name}-{subsection_name}
ì˜ì•½í’ˆ ìƒì‹ê³¼ ì œí’ˆëª… ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ 1-2ë¬¸ì¥ìœ¼ë¡œ ìƒì„±í•´ì£¼ì„¸ìš”. JSON í˜•ì‹: {{"{subsection_name}": "ë‚´ìš©"}}"""
        else:
            prompt = f"""ì œí’ˆëª…: {product_name}, í•­ëª©: {section_name}
ì˜ì•½í’ˆ ìƒì‹ê³¼ ì œí’ˆëª… ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ 1-2ë¬¸ì¥ìœ¼ë¡œ ìƒì„±í•´ì£¼ì„¸ìš”. JSON í˜•ì‹: {{"{section_name}": "ë‚´ìš©"}}"""
        
        print(f"ğŸ” {section_name}{' - ' + subsection_name if subsection_name else ''} í•­ëª© Ollama ìƒì„± ì¤‘...")
        
        result = client.generate_response(prompt)
        
        if result.get("error", False):
            print(f"   âŒ Ollama ì˜¤ë¥˜: {result.get('text', 'Unknown error')}")
            return f"{product_name}ì˜ {section_name}{' - ' + subsection_name if subsection_name else ''}ì— ëŒ€í•œ ì •ë³´ì…ë‹ˆë‹¤."
        
        response_text = result.get("text", "")
        if not response_text:
            return f"{product_name}ì˜ {section_name}{' - ' + subsection_name if subsection_name else ''}ì— ëŒ€í•œ ì •ë³´ì…ë‹ˆë‹¤."
        
        # JSON ì¶”ì¶œ
        json_result = client.extract_json_from_response(response_text)
        
        if json_result:
            if subsection_name and subsection_name in json_result:
                content = json_result[subsection_name]
                print(f"   âœ… {subsection_name} ìƒì„± ì„±ê³µ: {content[:50]}...")
                return content
            elif section_name in json_result:
                content = json_result[section_name]
                print(f"   âœ… {section_name} ìƒì„± ì„±ê³µ: {content[:50]}...")
                return content
        
        # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ë¬¸ì¥ ë°˜í™˜
        return f"{product_name}ì˜ {section_name}{' - ' + subsection_name if subsection_name else ''}ì— ëŒ€í•œ ì •ë³´ì…ë‹ˆë‹¤."
        
    except Exception as e:
        print(f"âŒ Ollama ë‚´ìš© ìƒì„± ì˜¤ë¥˜ ({section_name}): {e}")
        return f"{product_name}ì˜ {section_name}{' - ' + subsection_name if subsection_name else ''}ì— ëŒ€í•œ ì •ë³´ì…ë‹ˆë‹¤."

def create_korean_medicine_structure(product_name: str, generated_sentences: Dict[str, str]) -> Dict[str, Any]:
    """ìƒì„±ëœ ë¬¸ì¥ë“¤ì„ í•œêµ­ ì˜ì•½í’ˆ ì„¤ëª…ì„œ êµ¬ì¡°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤ (Ollama ìë™ ìƒì„± ë° í…ìŠ¤íŠ¸ ì •ì œ í¬í•¨)."""
    
    def get_content_or_generate(section_name: str, subsection_name: str = None) -> str:
        """ë‚´ìš©ì´ ì—†ìœ¼ë©´ Ollamaë¡œ ìƒì„±, ìˆìœ¼ë©´ ì •ì œ"""
        content = generated_sentences.get(section_name, "")
        if not content or content == "ì •ë³´ ì—†ìŒ" or "ì •ë³´ê°€ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤" in content:
            return generate_missing_content_with_ollama(product_name, section_name, subsection_name)
        else:
            # ê¸°ì¡´ ë‚´ìš©ì´ ìˆìœ¼ë©´ ì •ì œ
            return clean_and_improve_text_with_ollama(product_name, content, section_name, subsection_name)
    
    medicine_structure = {
        "ì œí’ˆëª…": product_name,
        "ì„±ë¶„ ë° í•¨ëŸ‰": [
            {
                "ì„±ë¶„ëª…": "ì£¼ì„±ë¶„",
                "ê·œê²©": get_content_or_generate("ì„±ë¶„ ë° í•¨ëŸ‰"),
                "ê¸°ì¤€": get_content_or_generate("ì„±ë¶„ ë° í•¨ëŸ‰", "ê¸°ì¤€")
            }
        ],
        "ì„±ìƒ": get_content_or_generate("ì„±ìƒ"),
        "íš¨ëŠ¥ ë° íš¨ê³¼": [
            get_content_or_generate("íš¨ëŠ¥ ë° íš¨ê³¼")
        ],
        "ìš©ë²• ë° ìš©ëŸ‰": [
            {
                "ì ì‘ì¦": get_content_or_generate("ìš©ë²• ë° ìš©ëŸ‰", "ì ì‘ì¦"),
                "ìš©ëŸ‰": get_content_or_generate("ìš©ë²• ë° ìš©ëŸ‰")
            }
        ],
        "ì‚¬ìš©ìƒ ì£¼ì˜ì‚¬í•­": {
            "ê²½ê³ ": [
                get_content_or_generate("ì‚¬ìš©ìƒ ì£¼ì˜ì‚¬í•­", "ê²½ê³ ")
            ],
            "ê¸ˆê¸°": [
                get_content_or_generate("ì‚¬ìš©ìƒ ì£¼ì˜ì‚¬í•­", "ê¸ˆê¸°")
            ],
            "ì£¼ì˜ í•„ìš” í™˜ì": [
                get_content_or_generate("ì‚¬ìš©ìƒ ì£¼ì˜ì‚¬í•­", "ì£¼ì˜ í•„ìš” í™˜ì")
            ],
            "ì´ìƒë°˜ì‘": [
                get_content_or_generate("ì‚¬ìš©ìƒ ì£¼ì˜ì‚¬í•­", "ì´ìƒë°˜ì‘")
            ]
        },
        "ìƒí˜¸ì‘ìš©": [
            get_content_or_generate("ìƒí˜¸ì‘ìš©")
        ],
        "ì„ë¶€ ë° ìˆ˜ìœ ë¶€ ì‚¬ìš©": {
            "ì„ì‹  1~2ê¸°": get_content_or_generate("ì„ë¶€ ë° ìˆ˜ìœ ë¶€ ì‚¬ìš©", "ì„ì‹  1~2ê¸°"),
            "ì„ì‹  3ê¸°": get_content_or_generate("ì„ë¶€ ë° ìˆ˜ìœ ë¶€ ì‚¬ìš©", "ì„ì‹  3ê¸°"),
            "ìˆ˜ìœ ë¶€": get_content_or_generate("ì„ë¶€ ë° ìˆ˜ìœ ë¶€ ì‚¬ìš©", "ìˆ˜ìœ ë¶€")
        },
        "ê³ ë ¹ì ì‚¬ìš©": get_content_or_generate("ê³ ë ¹ì ì‚¬ìš©"),
        "ì ìš© ì‹œ ì£¼ì˜ì‚¬í•­": [
            get_content_or_generate("ì ìš© ì‹œ ì£¼ì˜ì‚¬í•­")
        ],
        "ë³´ê´€ ë° ì·¨ê¸‰": {
            "ë³´ê´€ì¡°ê±´": get_content_or_generate("ë³´ê´€ ë° ì·¨ê¸‰", "ë³´ê´€ì¡°ê±´"),
            "í¬ì¥ë‹¨ìœ„": get_content_or_generate("ë³´ê´€ ë° ì·¨ê¸‰", "í¬ì¥ë‹¨ìœ„"),
            "ì£¼ì˜ì‚¬í•­": [
                get_content_or_generate("ë³´ê´€ ë° ì·¨ê¸‰", "ì£¼ì˜ì‚¬í•­")
            ]
        },
        "ì œì¡° ë° íŒë§¤ì‚¬ ì •ë³´": {
            "ì œì¡°ì‚¬": get_content_or_generate("ì œì¡° ë° íŒë§¤ì‚¬ ì •ë³´", "ì œì¡°ì‚¬"),
            "íŒë§¤ì‚¬": get_content_or_generate("ì œì¡° ë° íŒë§¤ì‚¬ ì •ë³´", "íŒë§¤ì‚¬"),
            "ê³µì¥ ì£¼ì†Œ": get_content_or_generate("ì œì¡° ë° íŒë§¤ì‚¬ ì •ë³´", "ê³µì¥ ì£¼ì†Œ"),
            "ì†Œë¹„ììƒë‹´ì‹¤": get_content_or_generate("ì œì¡° ë° íŒë§¤ì‚¬ ì •ë³´", "ì†Œë¹„ììƒë‹´ì‹¤")
        }
    }
    
    return medicine_structure

def create_korean_ctd_structure(product_name: str, generated_sentences: Dict[str, str]) -> Dict[str, Any]:
    """ê¸°ì¡´ CTD êµ¬ì¡° (í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€)"""
    return create_korean_medicine_structure(product_name, generated_sentences)

# ê¸°ì¡´ í•¨ìˆ˜ë“¤ì€ í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€í•˜ë˜ ìƒˆë¡œìš´ êµ¬ì¡°ë¥¼ ì‚¬ìš©í•˜ë„ë¡ ìˆ˜ì •
def split_into_blocks(text: str) -> List[str]:
    """í…ìŠ¤íŠ¸ë¥¼ ë¬¸ë‹¨ ë‹¨ìœ„ë¡œ ë¶„í• í•©ë‹ˆë‹¤. (í˜¸í™˜ì„± ìœ ì§€)"""
    return split_into_tokens(text)  # ìƒˆë¡œìš´ êµ¬ì¡° ì‚¬ìš©

def split_text_into_chunks(text: str, chunk_size: int = 300, overlap: int = 50) -> List[str]:
    """í…ìŠ¤íŠ¸ë¥¼ ì‘ì€ ì²­í¬ë¡œ ë¶„í• í•©ë‹ˆë‹¤. (í˜¸í™˜ì„± ìœ ì§€)"""
    return split_into_tokens(text)  # ìƒˆë¡œìš´ êµ¬ì¡° ì‚¬ìš©

def extract_medical_data_from_text_with_chunks(text: str, user_product_name: str, chunk_size: int = 200, overlap_size: int = 30) -> Dict[str, Any]:
    """ì²­í¬ ê¸°ë°˜ìœ¼ë¡œ í…ìŠ¤íŠ¸ì—ì„œ ì˜ì•½í’ˆ ê´€ë ¨ ë°ì´í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤. (í˜¸í™˜ì„± ìœ ì§€)"""
    # ìƒˆë¡œìš´ êµ¬ì¡° ì‚¬ìš© (ì²­í¬ ì„¤ì •ì€ ë¬´ì‹œí•˜ê³  í† í° ë‹¨ìœ„ ì‚¬ìš©)
    return extract_medical_data_from_text(text, user_product_name) 